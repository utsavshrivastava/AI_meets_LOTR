{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5emHeseBVMQa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BsPHOG0eVRfS"
   },
   "outputs": [],
   "source": [
    "LAYERS_COUNT = 4\n",
    "LSTM_DIM = 256\n",
    "TEMPERATURE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "24zmK_OQVibw"
   },
   "outputs": [],
   "source": [
    "with open(\"./data/complete_lotr.txt\", \"rb\") as f:\n",
    "    text = f.read().decode(encoding='utf-8')\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "char_to_idx = {c:i for i,c in enumerate(vocab)}\n",
    "idx_to_char = {i:c for c,i in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MzCa723pg0LO"
   },
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "\n",
    "testing_model = tf.keras.models.Sequential()\n",
    "for i in range(LAYERS_COUNT):\n",
    "    testing_model.add(\n",
    "            LSTM(\n",
    "                LSTM_DIM, \n",
    "                return_sequences=True if (i!=(LAYERS_COUNT-1)) else False,\n",
    "                batch_input_shape=(1, 1, vocab_size),\n",
    "                stateful=True\n",
    "            )\n",
    "        )\n",
    "testing_model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.01)\n",
    "testing_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "testing_model.load_weights(filepath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iW9EPc2ShX9D"
   },
   "outputs": [],
   "source": [
    "def gumbel_sample(probs, temperature = TEMPERATURE):\n",
    "  \"\"\"Helper function to sample an index from a probability array\"\"\"\n",
    "  # from fchollet/keras\n",
    "  probs = np.asarray(probs).astype('float64')\n",
    "  probs = np.log(probs) / temperature\n",
    "  exp_preds = np.exp(probs)\n",
    "  probs = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, probs, 1)\n",
    "  return np.argmax(probas)\n",
    "\n",
    "def predict_next_char(model, current_char):\n",
    "  x = np.zeros((1, 1, vocab_size))\n",
    "  x[:,:,char_to_idx[current_char]] = 1\n",
    "  y = model.predict(x, batch_size=1)\n",
    "  next_char_idx = gumbel_sample(y[0,:])\n",
    "  next_char = idx_to_char[next_char_idx]\n",
    "  return next_char\n",
    "\n",
    "def generate_text(model, seed, length):\n",
    "    \"\"\"Generate characters from a given seed\"\"\"\n",
    "    generated_text = seed\n",
    "    model.reset_states()\n",
    "    for c in seed[:-1]:\n",
    "        next_char = predict_next_char(model, c)\n",
    "    current_char = seed[-1]\n",
    "\n",
    "    for i in range(length - len(seed)):\n",
    "        next_char = predict_next_char(model, current_char)\n",
    "        current_char = next_char\n",
    "        generated_text += current_char\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfRcNKYxm_h7",
    "outputId": "764518c4-e25c-4b45-926b-31de6c8e67bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1 \n",
      "\n",
      "On the way to Mordor before the Shirriff, very scent. The West was ever been the basin miles and pockets called or a right to the way in a great sat more about him and had seen in her later eastward, \n",
      "\n",
      "\n",
      "Temperature: 0.8 \n",
      "\n",
      "On the way to Mordor, and the air no one was desperately. 'Oh I want now!' He said to himself merry.\n",
      "     But it was a white grey foes of falling like some bullies were looking to the Morgul Towers t\n",
      "\n",
      "\n",
      "Temperature: 1.0 \n",
      "\n",
      "On the way to Mordor guessed silence. In thought and the peril across the marrion of the crown, and a whole will that grew hand; and the shadows of the battle was to like the Sun tears out of the sun \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generate_text(testing_model, seed = \"The ring\", length = 400)\n",
    "gen_text_length = 200\n",
    "seed_list = [\"On the way to Mordor\"]\n",
    "for temperature in [.1, .8,1.0]:\n",
    "    for starting in seed_list:\n",
    "        print(f\"Temperature: {temperature} \\n\")\n",
    "        generate_text(testing_model, seed = starting, length = gen_text_length)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
